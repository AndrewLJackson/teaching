---
title: "Polynomials Tutorial Work-through"
author: "Andrew L Jackson"
date: "24 November 2015"
output: html_document
---

This document works through the tasks set in the related 
"intro-to-model-selection-with-polynomials" document. We fit various orders of 
polynomial to a dataset and compare them using AIC.

First, the data:

```{r, echo = TRUE}

# required packages
library(viridis) # for accessible colourmaps
palette(viridis(8))

mydata <- read.csv("curved-data.csv", header = T, stringsAsFactors = FALSE)

plot(richness ~ effort, data = mydata, bty = "L", cex.lab = 1.2, 
     xlab = "Fishing Effort (time spent hauling in minutes)",
     ylab = "species richness")

```

Now we start to build the various expononents of `effort` so we can fit various 
orders of polynomial.


```{r, echo = TRUE}

# specify up to 8th order of exponents
mydata$effort.2 <- mydata$effort ^ 2
mydata$effort.3 <- mydata$effort ^ 3
mydata$effort.4 <- mydata$effort ^ 4
mydata$effort.5 <- mydata$effort ^ 5
mydata$effort.6 <- mydata$effort ^ 6
mydata$effort.7 <- mydata$effort ^ 7
mydata$effort.8 <- mydata$effort ^ 8
mydata$effort.9 <- mydata$effort ^ 9
mydata$effort.10 <- mydata$effort ^ 10

# specify all our models from 0 to 8th order polynomial
m0 <- glm(richness ~ 1, data = mydata)

m1 <- glm(richness ~ effort, data = mydata)

m2 <- glm(richness ~ effort + effort.2, data = mydata)

m3 <- glm(richness ~ effort + effort.2 + effort.3, data = mydata)

m4 <- glm(richness ~ effort + effort.2  + effort.3 + effort.4, data = mydata)

m5 <- glm(richness ~ effort + effort.2  + effort.3 + effort.4 +
            effort.5, data = mydata)

m6 <- glm(richness ~ effort + effort.2  + effort.3 + effort.4 
          + effort.5 + effort.6, data = mydata)

m7 <- glm(richness ~ effort + effort.2  + effort.3 + effort.4 
          + effort.5 + effort.6 + effort.7, data = mydata)

m8 <- glm(richness ~ effort + effort.2  + effort.3 + effort.4 
          + effort.5 + effort.6 + effort.7 + effort.8, data = mydata)

m9 <- glm(richness ~ effort + effort.2  + effort.3 + effort.4 
          + effort.5 + effort.6 + effort.7 + effort.8 + effort.9, data = mydata)

m10 <- glm(richness ~ effort + effort.2  + effort.3 + effort.4 
          + effort.5 + effort.6 + effort.7 + effort.8 + effort.9
          + effort.10, data = mydata)


# create a table of the AIC values with AIC values rounded to 
# one decimal place.
aic.table <- data.frame(polynomial.order = 0:10, 
                        AIC = round(
                              c(extractAIC(m0)[2],
                                extractAIC(m1)[2],
                                extractAIC(m2)[2],
                                extractAIC(m3)[2],
                                extractAIC(m4)[2],
                                extractAIC(m5)[2],
                                extractAIC(m6)[2],
                                extractAIC(m7)[2],
                                extractAIC(m8)[2],
                                extractAIC(m9)[2],
                                extractAIC(m10)[2]
                                ) 
                              , digits = 1 )
                       )
                      
print(aic.table, row.names = F)


```

So, quite a bit of tedious code to acheive that, but most of it is a 
*copy-paste-edit-repeat* task which is not so bad. We can see that the 
AIC of the polynomials drops very quickly as the order increases, corresponding 
with a large increase in the likelihood of the data given the model 
(i.e. model fit). It then starts to bottom out around order of 7 before increasing again 
when the increase in loglikelihood is no longer greater than the -2 penalty applied for 
adding an additional parameter: recall that $AIC = -2*\log(likelihood) + 2 * k$, where 
$k$ is the number of parameters.

We could even visualise this model comparison in a plot:

```{r, echo=T}

par(mfrow=c(1,1))
plot(AIC ~ polynomial.order, data = aic.table, type = "b", 
     bty = "L", xlab = "Polynomial Order", ylab = "AIC",
     cex.lab = 1.2, cex.axis = 1.2, col = 1)

```

The "*rule*" for model selection is that we only add an additional parameter if the AIC 
drops by more than $2$ units. This reflects the concept of 
[parsimony]("https://en.wikipedia.org/wiki/Occam%27s_razor"), and effectively means you 
only make a model more complicated (by adding an additional parameter) if the increase in 
log(liklihood) is more than the cost of the additional parameter.

Using this *rule*, we can conclude that AIC supports a 5th order polynomial as best explaining 
the data. Although the 7th order polynomial has a lower AIC, it has two more parameters, and so 
a reduction in AIC of greater than 4 units would be required to justify their inclusion.

## Visualising our chosen model - 5th order polynomial

Bearing in mind this is just an excercise, and in reality the model 
$richness ~ log(effort)$ is actually a much more parsimonious and easy to 
explain model of these data... we can none-the-less visualise our 5th order 
polynomial and check is suitability. Just because it has the lowest AIC does not
mean that the model itself is any good; we have to do some checking of the model 
fit by inspecting the residuals etc...

```{r, echo=T}

new.effort <- seq(0, 100, 1)

new.data <- data.frame(effort = new.effort,
                       effort.2 = new.effort ^ 2,
                       effort.3 = new.effort ^ 3,
                       effort.4 = new.effort ^ 4,
                       effort.5 = new.effort ^ 5)

y.hat <- predict(m5, newdata = new.data)

par(mfrow=c(1,1))

# plot the raw data
plot(richness ~ effort, data = mydata, bty = "L", cex.lab = 1.2, 
     xlab = "Fishing Effort (time spent hauling in minutes)",
     ylab = "species richness")

# add the 5th order polynomial
lines(y.hat ~ new.effort, col = 6, lty = 1, lwd = 2)

# and check the residuals for normality with a histogram
hist(resid(m5), 20, col = 6)

# check for normality of the residuals using a qq-plot
# which shows how the estimated residuals from the model, line 
# up with a theoretically perfect normal distribution.
qqnorm(resid(m5))
qqline(resid(m5))

# check for a trend in the residuals with the explanatory variable
plot(resid(m5) ~ effort, data = mydata)
abline(h = 0, lwd = 2, lty = 2, col = 6)



```

Ultimately, this 5th order polynomial does a pretty great job and describing the data in a 
visual sense. But, the problem is that the model offers little or no insight into whats 
going on in the data; we cant for instance look at the table of esimated coefficients in our 
glm output and say anything immediate about the how species richness changes with 
sampling effort. In contrast, fitting the model $richness ~ log(effort)$ results in a simple 
intercept and slope with which we can make completely intelligible statements about our data.

We can leave that interpretation to another day, but basically, as the response variable is 
on its raw scale, and the explanatory variable on a log scale, the intrepretation will be: 
a proportional change in effort results in a linear increase in richess of some amount.

In class we discussed why richness should asymptote to the *true species richness* for an area, 
and model $richness ~ log(effort)$ still doesnt satisfy this mechanistic understanding of the 
process. So, if you really want to fit an asympototic model, then you need to use a different 
transformation or use a [non-linear statistical model]("https://en.wikipedia.org/wiki/Nonlinear_regression") that can fit more complex equations such 
as [sigmoidal curves]("https://en.wikipedia.org/wiki/Sigmoid_function").
